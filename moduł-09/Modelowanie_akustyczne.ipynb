{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sages-Modelowanie akustyczne.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsStfsHKieLV"
      },
      "source": [
        "# Podstawy modelowania akustycznego\n",
        "\n",
        "Ten notatnik zawiera instrukcje na temat modelowania akustycznego w rozpoznawaniu mowy.\n",
        "\n",
        "Poniższe polecenie importuje numpy i matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odbc6gR3T5y8"
      },
      "source": [
        "%pylab inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj8CylTJi8vB"
      },
      "source": [
        "Do obługi sieci neuronowych użyjemy biblioteki Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk0vs6TTUfcP"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.utils import to_categorical, plot_model\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSVDrGSXjDN0"
      },
      "source": [
        "Wgrywamy następujące pliki i programy:\n",
        "\n",
        "- clarin_small.h5 - podzbiór większego korpusu mowy. Plik ten zawiera cechy MFCC zapisane w formacie HDF5\n",
        "- indices_small.pkl - indeksy do podziału tego zbioru na `train`/`valid`/`test` w formacie pickle\n",
        "- phones.txt - lista fonemów użyta w pliku HDF5 (tam dla oszczędności użyto indeksy zamiast string'i)\n",
        "- data.py - program do wczytywania pliku HDF5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMHQuK2tkMeP"
      },
      "source": [
        "# Wczytanie danych\n",
        "\n",
        "Importujemy klasę Corpus z modułu data i wczytujemy plik 'clarin_small.h5'. Ustawiamy argumenty `load_normalized=True` oraz `merge_utts=False`. Po skonstruowaniu korpusu, używamy metodę `get()` żeby pobrać wsyzstkie dane. \n",
        "\n",
        "Plik `phones.txt` wczytujemy do mapy fonemów.\n",
        "\n",
        "Plik `indices_small.pkl` zawiera podział na dane `train`/`dev`/`test`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPFDX1tNVhzS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61iW_b3omTKF"
      },
      "source": [
        "## Zmienne globalne\n",
        "\n",
        "- input_dim - ilość cech w jednej ramce - można wyliczyć z którejś tablicy X\n",
        "- output_dim - ilość różnych fonemów - można wyliczyć z rozmiaru mapy fonemów\n",
        "- hidden_num - szerokość wartswy ukrytej modelu (np 256)\n",
        "- batch_size - ilość próbek w jednym batchu (np 256)\n",
        "- epoch_num - ilość epok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JYevHm8ZFe8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDMMIVRRmSqp"
      },
      "source": [
        "Robimy `tr_`/`dev_`/`tst_` podzbiór `X` i `y`.\n",
        "\n",
        "Do `y` użyjemy `to_categorical`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSt34kjKmSXx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T28zibQ8nqJi"
      },
      "source": [
        "## Model\n",
        "\n",
        "Stwórz model typu `Sequential` i warstwy:\n",
        "\n",
        "1. `dense` z `input_dim` = `input_dim`, `units` = `hidden_num`, funkcja aktywacji `sigmoid`.\n",
        "2. `dense` z `units` = `output_dim`, funkcja aktywacji `softmax`\n",
        "\n",
        "Dodatkowo:\n",
        "\n",
        "- `optimizer` = `Adadelta()`\n",
        "- `loss` = `categorical_crossentropy`\n",
        "- `metrics` = `['acc']`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyYNBvGfbKVS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vJE1hTZpEE0"
      },
      "source": [
        "## Trenowanie\n",
        "\n",
        "Uruchamiamy `fit`:\n",
        "\n",
        "- `X` i 'y` zbioru treningowego\n",
        "- parametr `shuffle` ustawiamy na `True`\n",
        "- parametr `batch_size`\n",
        "- parametr `epochs` ustawiamy na `epoch_num`\n",
        "- parametr `verbose` ustawiamy na 1\n",
        "- parametr `validation_data` na tuple danych walidacyjnych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrT9glwtdwHI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8E6UrQkqXjh"
      },
      "source": [
        "## Ewaluacja\n",
        "\n",
        "Używamy metody `evaluate` na zbiorze testowym:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO_Hio-igvjN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCIIThPYqthZ"
      },
      "source": [
        "Używamy `predict_classes` i rysujemy macierz konfuzji:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h49RY7PHiQIv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTyjYUmVMThN"
      },
      "source": [
        "# Pomysły na rozszerzenie\n",
        "\n",
        "* testy hiperparametrów: ilość wartstw, rozmiary warstw, funkcje optymalizacyjne\n",
        "* inne rodzaje warstw i funkcje optymalizacji: dropout, tanh, relu, ...\n",
        "* kontekst danych wejściowych\n",
        "* sieci splotowe\n",
        "* sieci rekurencyjne\n",
        "* trenowanie aklgorytmem CTC zamiast klasyfikacji"
      ]
    }
  ]
}